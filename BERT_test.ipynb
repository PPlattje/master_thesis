{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y84RRpp39SOc",
        "colab_type": "text"
      },
      "source": [
        "Script that evaluates a fine-tuned a BERT language model to evaluate created datasets\n",
        "\n",
        "\n",
        "based on https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltzx5JRi_RQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "!ls \"drive/My Drive/Colab Notebooks/abusive\"\n",
        "!pip install transformers==2.1.1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBJZYMi0_XMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import torch\n",
        "import pickle\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from tqdm import tqdm, trange\n",
        "import os\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
        "from transformers.optimization import AdamW, WarmupLinearSchedule\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import numpy as np\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
        "\n",
        "logger = logging.getLogger()\n",
        "def get_eval_report(task_name, labels, preds):\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
        "    pos_prec = tp / (tp +fp) \n",
        "    pos_reca = tp / (tp + fn)\n",
        "    neg_prec = tn / (tn + fn)\n",
        "    neg_reca = tn / (tn + fp)\n",
        "    pos_f1 = (2 * ((pos_prec * pos_reca) / (pos_prec + pos_reca)))\n",
        "    neg_f1 = (2 * ((neg_prec * neg_reca) / (neg_prec + neg_reca)))\n",
        "    macro_f1 = (pos_f1 + neg_f1) / 2\n",
        "    return {\n",
        "        \"task\": task_name,\n",
        "        \"mcc\": mcc,\n",
        "        \n",
        "        \"tp\": tp,\n",
        "        \"tn\": tn,\n",
        "        \"fp\": fp,\n",
        "        \"fn\": fn,\n",
        "        \"macro_f1\": macro_f1\n",
        "    }\n",
        "\n",
        "def collect_wrong_preds(labels,preds):\n",
        "    wronglist = []\n",
        "    assert len(preds) == len(labels)\n",
        "    for x in range(len(preds)):\n",
        "        if preds[x] != labels[x]:\n",
        "            wronglist.append(x)\n",
        "    return wronglist\n",
        "    \n",
        "def compute_metrics(task_name, labels, preds):\n",
        "    assert len(preds) == len(labels)\n",
        "    return get_eval_report(task_name, labels, preds)\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class BinaryClassificationProcessor(DataProcessor):\n",
        "    \"\"\"Processor for binary classification dataset.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[3]\n",
        "            label = line[1]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "def convert_example_to_feature(example_row):\n",
        "    # return example_row\n",
        "    example, label_map, max_seq_length, tokenizer, output_mode = example_row\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "    segment_ids = [0] * len(tokens)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    padding = [0] * (max_seq_length - len(input_ids))\n",
        "    input_ids += padding\n",
        "    input_mask += padding\n",
        "    segment_ids += padding\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    if output_mode == \"classification\":\n",
        "        label_id = label_map[example.label]\n",
        "    elif output_mode == \"regression\":\n",
        "        label_id = float(example.label)\n",
        "    else:\n",
        "        raise KeyError(output_mode)\n",
        "\n",
        "    return InputFeatures(input_ids=input_ids,\n",
        "                         input_mask=input_mask,\n",
        "                         segment_ids=segment_ids,\n",
        "                         label_id=label_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5N_kyZQAVV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BERT_MODEL = 'bert-base-cased'\n",
        "TASK_NAME = '' # fill in task name\n",
        "BASE_PATH =\"drive/My Drive/Colab Notebooks/experiments\"\n",
        "OUTPUT_DIR = f'{BASE_PATH}/outputs/{TASK_NAME}/'\n",
        "REPORTS_DIR = f'{BASE_PATH}/reports/{TASK_NAME}_evaluation_report/'\n",
        "DATA_DIR = f'{BASE_PATH}/data'\n",
        "CACHE_DIR = 'cache/'\n",
        "\n",
        "MAX_SEQ_LENGTH = 128\n",
        "TRAIN_BATCH_SIZE = 24\n",
        "EVAL_BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 10\n",
        "RANDOM_SEED = 42\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "WARUMUP_STEPS = 100\n",
        "\n",
        "OUTPUT_MODE = 'classification'\n",
        "CONFIG_NAME = \"config.json\"\n",
        "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
        "\n",
        "output_mode = OUTPUT_MODE\n",
        "cache_dir = CACHE_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUUR_O1-8TeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_REPORTS_DIR = f'{BASE_PATH}/reports/{TASK_NAME}_evaluation_report/test'\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "processor = BinaryClassificationProcessor()\n",
        "if not os.path.exists(BASE_REPORTS_DIR):\n",
        "    os.makedirs(BASE_REPORTS_DIR)\n",
        "evaldata = ['new/abusive_sieve'] #select which testing dataset(s) to evaluate on\n",
        "\n",
        "for dataset in evaldata:\n",
        "    DATA_DIR = f'{BASE_PATH}/data/BERT/{dataset}'\n",
        "    REPORTS_DIR = f'{BASE_REPORTS_DIR}/{dataset}'\n",
        "    if not os.path.exists(REPORTS_DIR):\n",
        "        print('created ' + REPORTS_DIR)\n",
        "        os.makedirs(REPORTS_DIR)\n",
        "    print(f'reports dir: {REPORTS_DIR}')\n",
        "    print(f'data dir: {DATA_DIR}')\n",
        "\n",
        "\n",
        "    eval_examples = processor.get_dev_examples(DATA_DIR)\n",
        "    label_list = processor.get_labels()\n",
        "    num_labels = len(label_list)\n",
        "    eval_examples_len = len(eval_examples)\n",
        "\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "    eval_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in eval_examples]\n",
        "\n",
        "    process_count = cpu_count() - 1\n",
        "    print(f'Preparing to convert {eval_examples_len} examples..')\n",
        "    print(f'Spawning {process_count} processes..')\n",
        "    with Pool(process_count) as p:\n",
        "        eval_features = list(tqdm(p.imap(convert_example_to_feature, eval_examples_for_processing), total=eval_examples_len))\n",
        "    with open(DATA_DIR + \"/eval_features.pkl\", \"wb\") as f:\n",
        "        pickle.dump(eval_features, f)\n",
        "    with open(DATA_DIR + \"/eval_features.pkl\", 'rb') as pkl_file:\n",
        "        train_features= pickle.load(pkl_file)\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "    \n",
        "    if OUTPUT_MODE == \"classification\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
        "    elif OUTPUT_MODE == \"regression\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.float)\n",
        "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    eval_sampler = SequentialSampler(eval_data)\n",
        "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=EVAL_BATCH_SIZE)\n",
        "\n",
        "    for epoch in range(1,6):\n",
        "        print('loading model')\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-cased', cache_dir=CACHE_DIR, num_labels=len(label_list))\n",
        "        model_to_load = 'drive/My Drive/Colab Notebooks/experiments/outputs/{0}/epoch_{1}/pytorch_model.bin'.format(TASK_NAME,epoch)\n",
        "        print(model_to_load)\n",
        "        model.load_state_dict(torch.load(model_to_load))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        preds = []\n",
        "        print(len(eval_dataloader))\n",
        "        for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = input_ids.to(device)\n",
        "            input_mask = input_mask.to(device)\n",
        "            segment_ids = segment_ids.to(device)\n",
        "            label_ids = label_ids.to(device)\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_ids, segment_ids, input_mask, labels=None)[0]\n",
        "\n",
        "            if OUTPUT_MODE == \"classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
        "            elif OUTPUT_MODE == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "            if len(preds) == 0:\n",
        "                preds.append(logits.detach().cpu().numpy())\n",
        "            else:\n",
        "                preds[0] = np.append(\n",
        "                    preds[0], logits.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        preds = preds[0]\n",
        "        if OUTPUT_MODE == \"classification\":\n",
        "            preds = np.argmax(preds, axis=1)\n",
        "        elif OUTPUT_MODE == \"regression\":\n",
        "            preds = np.squeeze(preds)\n",
        "        labels = all_label_ids.numpy()\n",
        "        result = compute_metrics(TASK_NAME, labels, preds)\n",
        "        result['eval_loss'] = eval_loss\n",
        "\n",
        "        output_eval_file = os.path.join(REPORTS_DIR, \"eval_results_epoch_{0}.txt\".format(epoch))\n",
        "        output_preds_file = os.path.join(REPORTS_DIR, \"preds_epoch_{0}.txt\".format(epoch))\n",
        "        with open(output_preds_file, 'w') as csvfile:\n",
        "            predwriter = csv.writer(csvfile, delimiter='\\t')\n",
        "            assert len(preds) == len(labels)\n",
        "            predwriter.writerow(['tweet', 'prediction', 'gold standard'])\n",
        "            for x in range(len(preds)):\n",
        "                predwriter.writerow([eval_examples[x].text_a, preds[x], labels[x]])\n",
        "        print(output_eval_file)\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            print(\"***** Eval results *****\")\n",
        "            for key in (result.keys()):\n",
        "                print(\"  %s = %s\", key, str(result[key]))\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}