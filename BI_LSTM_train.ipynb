{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BI-LSTM_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JpSMQLR-sV_",
        "colab_type": "text"
      },
      "source": [
        "BI-LSTM Model training based on polarised or pre-trained embeddings\n",
        "\n",
        "Based on the following tutorials:\n",
        "\n",
        "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
        "\n",
        "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ob4cn8j2nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!ls \"drive/My Drive/Colab Notebooks\" \n",
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8k94YOoRqbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import random\n",
        "import time\n",
        "from torchtext import data\n",
        "from torchtext.vocab import Vectors\n",
        "import fasttext\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUMyhGg4RvYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_accuracy(preds, gold):\n",
        "    \"\"\"returns accuracy per batch.\"\"\"\n",
        "    #round to closest integer, cause binary classification\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = 0\n",
        "    for x in range(len(rounded_preds)):\n",
        "        if rounded_preds[x] == gold[x]:\n",
        "            correct += 1\n",
        "        \n",
        "    acc = correct / len(rounded_preds)\n",
        "    return acc\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"count number of trainable parameters\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def progress_tracker(total, current, start_time):\n",
        "    per = (current / total) * 100\n",
        "    global milestone\n",
        "    cminutes, cseconds = epoch_time(start_time, time.time())\n",
        "    ctime = '{0}m {1}s'.format(cminutes,cseconds)\n",
        "    if per > 90:\n",
        "        if milestone != 90:\n",
        "            print('Progress training: 90% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 90\n",
        "    elif per > 80:\n",
        "        if milestone != 80:\n",
        "            print('Progress training: 80% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 80\n",
        "    elif per > 70:\n",
        "        if milestone != 70:\n",
        "            print('Progress training: 70% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 70\n",
        "    elif per > 60:\n",
        "        if milestone != 60:\n",
        "            print('Progress training: 60% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 60\n",
        "    elif per > 50:\n",
        "        if milestone != 50:\n",
        "            print('Progress training: 50% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 50\n",
        "    elif per > 40:\n",
        "        if milestone != 40:\n",
        "            print('Progress training: 40% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 40\n",
        "    elif per > 30:\n",
        "        if milestone != 30:\n",
        "            print('Progress training: 30% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 30\n",
        "    elif per > 20:\n",
        "        if milestone != 20:\n",
        "            print('Progress training: 20% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 20\n",
        "    elif per > 10:\n",
        "        if milestone != 10:\n",
        "            print('Progress training: 10% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 10\n",
        "\n",
        "\n",
        "def calc_metrics(preds, y):\n",
        "    \"\"\"returns F1 score per batch.\"\"\"\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    tn = 0\n",
        "    for x in range(len(rounded_preds)):\n",
        "        if rounded_preds[x] == 1:\n",
        "            if y[x] == 1:\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            if y[x] == 1:\n",
        "                fn += 1\n",
        "            if y[x] == 0:\n",
        "                tn += 1\n",
        "    return tp, fp, fn, tn\n",
        "            \n",
        "   \n",
        "def train(model, train_iterator, optimizer, criterion, start_time):\n",
        "    \"\"\"train the model\"\"\"\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train() # activate  training\n",
        "    progress = 0\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    for batch in train_iterator:\n",
        "        optimizer.zero_grad() #reset gradient\n",
        "        text, text_lengths = batch.text\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = calc_accuracy(predictions, batch.label)\n",
        "        batch_tp, batch_fp, batch_fn, batch_tn = calc_metrics(predictions, batch.label)\n",
        "        tn += batch_tn\n",
        "        tp += batch_tp\n",
        "        fp += batch_fp\n",
        "        fn += batch_fn\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "        progress += len(batch)\n",
        "        prevbatch = batch\n",
        "       # progress_tracker(len(train_iterator), progress, start_time)\n",
        "    if tp != 0:    \n",
        "        prec = tp / (tp + fp)\n",
        "        reca = tp / (tp +fn)\n",
        "    else:\n",
        "        prec = 'no TP'\n",
        "        reca = 'no TP'\n",
        "    \n",
        "    # return average loss and acc over all batches    \n",
        "    print(len(train_iterator))\n",
        "    return epoch_loss / len(train_iterator), epoch_acc / len(train_iterator), prec, reca\n",
        "\n",
        "    \n",
        "def evaluate(model, dev_iterator, criterion):\n",
        "    \"\"\"evaluate the model (no updateing model) \"\"\"\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    model.eval() #deactivate training (no updating model)\n",
        "    with torch.no_grad():\n",
        "        for batch in dev_iterator:\n",
        "            text, text_lengths = batch.text\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = calc_accuracy(predictions, batch.label)\n",
        "            batch_tp, batch_fp, batch_fn, batch_tn = calc_metrics(predictions, batch.label)\n",
        "            tn += batch_tn\n",
        "            tp += batch_tp\n",
        "            fp += batch_fp\n",
        "            fn += batch_fn\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc\n",
        "    if tp != 0:    \n",
        "        prec = tp / (tp + fp)\n",
        "        reca = tp / (tp +fn)\n",
        "    else:\n",
        "        prec = 'no TP (tp:{0},fp:{1},tn:{2},fn:{3})'.format(tp,fp,tn,fn)\n",
        "        reca = 'no TP'\n",
        "    return epoch_loss / len(dev_iterator), epoch_acc / len(dev_iterator), prec, reca, tn, tp, fp, fn\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "    \n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    \"\"\"BI-LSTM Neural Network Model\"\"\"\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx): \n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True,\n",
        "                           num_layers=n_layers, dropout=dropout)\n",
        "        self.out = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #packing ensured we do not process the the padded elements\n",
        "        # this makes it faster\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout       \n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))          \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "        return self.out(hidden.squeeze(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nq9nIObRz8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_PATH =\"drive/My Drive/Colab Notebooks/experiments\"\n",
        "TASK_NAME = '' #give up name of task\n",
        "OUTPUT_DIR = f'{BASE_PATH}/outputs/{TASK_NAME}/'\n",
        "BASE_REPORTS_DIR = f'{BASE_PATH}/reports/{TASK_NAME}_evaluation_report'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "if not os.path.exists(BASE_REPORTS_DIR):\n",
        "    os.makedirs(BASE_REPORTS_DIR)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "SEED = 111\n",
        "torch.manual_seed(SEED)\n",
        "TEXT = data.Field(include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "datafields = [(\"text\", TEXT),\n",
        "             (\"label\", LABEL)]\n",
        "train_data  = data.TabularDataset( #select right training data\n",
        "            path=\"drive/My Drive/Colab Notebooks/experiments/data/fastexttagger/abusivetrain.tsv\",\n",
        "            format=\"tsv\", fields=datafields)\n",
        "dev_data  = data.TabularDataset( # select right development data\n",
        "            path=\"drive/My Drive/Colab Notebooks/experiments/data/fastexttagger/abusivedev.tsv\",\n",
        "            format=\"tsv\", fields=datafields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm6NmxiT_x3Q",
        "colab_type": "text"
      },
      "source": [
        "Code used to approximate embeddings for OOV words from the training data. Only run first time: new embeddings will be saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEnL-BOINaA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fasttextmodel = fasttext.load_model(\"drive/My Drive/Colab Notebooks/embeddings/badwordembeddingsxxl.bin\")\n",
        "#wordlist = []\n",
        "#with open('embeddings.vec', 'w') as f:\n",
        "#    for x in range(len(train_data)):\n",
        "#        for word in train_data[x].text:\n",
        "#            if word not in wordlist:\n",
        "#                vec = [x for x in fasttextmodel[word]]\n",
        "#                print(word + ' ' + (' '.join(str(x) for x in vec)), file=f)\n",
        "#                wordlist.append(word)                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-twGdKKf__k8",
        "colab_type": "text"
      },
      "source": [
        "Select pre-trained or polarised embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygik2CYsNQqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('loading vectors')\n",
        "#pre-trained embeddings:\n",
        "vec = Vectors(name='pretrained_embeddings.vec', cache='drive/My Drive/Colab Notebooks/embeddings')\n",
        "\n",
        "#polarised embeddings:\n",
        "#vec = Vectors(name='drive/My Drive/Colab Notebooks/embeddings/abusive_embeddings.vec', cache='/content/')\n",
        "\n",
        "print('building vocabulary')\n",
        "TEXT.build_vocab(train_data, unk_init = torch.Tensor.normal_,\n",
        "                 vectors = vec) \n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTqvc40dR4Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab) \n",
        "EMBEDDING_DIM = 300 #should be same as embedding dimensions\n",
        "HIDDEN_DIM = 300 \n",
        "OUTPUT_DIM = 1 \n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "print('creating model')\n",
        "model = Classifier(INPUT_DIM,EMBEDDING_DIM,HIDDEN_DIM,OUTPUT_DIM,N_LAYERS,DROPOUT,PAD_IDX)\n",
        "model.to(device)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "print('preparing model')\n",
        "\n",
        "\n",
        "print('loading embeddings')\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = criterion.to(device)\n",
        "BATCH_SIZE = 24\n",
        "train_iterator, dev_iterator = data.BucketIterator.splits(\n",
        "    (train_data, dev_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml2xnpnTR8E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20\n",
        "if not os.path.exists(BASE_REPORTS_DIR):\n",
        "    os.makedirs(BASE_REPORTS_DIR)\n",
        "for epoch in range(EPOCHS):\n",
        "    milestone = 0\n",
        "    start_time = time.time()\n",
        "    print('training...')\n",
        "    train_loss, train_acc, train_prec, train_reca  = train(model, train_iterator, optimizer, criterion, start_time)\n",
        "    print('testing on development set...')\n",
        "    dev_loss, dev_acc, dev_prec, dev_reca, tn, tp, fp, fn = evaluate(model, dev_iterator, criterion)\n",
        "    try:\n",
        "        pos_prec = tp / (tp +fp) \n",
        "        pos_reca = tp / (tp + fn)\n",
        "        neg_prec = tn / (tn + fn)\n",
        "        neg_reca = tn / (tn + fp)\n",
        "        pos_f1 = (2 * ((pos_prec * pos_reca) / (pos_prec + pos_reca)))\n",
        "        neg_f1 = (2 * ((neg_prec * neg_reca) / (neg_prec + neg_reca)))\n",
        "        macro_f1 = (pos_f1 + neg_f1) / 2\n",
        "    except ZeroDivisionError:\n",
        "        macro_f1 = 'zerodevision'\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    etime = '{0}m {1}s'.format(epoch_mins,epoch_secs)\n",
        "    \n",
        "    torch.save(model.state_dict(), f'{OUTPUT_DIR}model_{epoch}.pt')\n",
        "\n",
        "    print('Epoch: {0} / {1} | Epoch Time: {2}'.format(epoch + 1, EPOCHS, etime))\n",
        "    print('Train Loss: {0} | Train Acc: {1}'.format(train_loss, train_acc * 100))\n",
        "    print('Train Pos Prec: {0} | Train Pos Reca: {1}'.format(train_prec, train_reca))\n",
        "    print('dev Loss: {0} | dev Acc: {1}'.format(dev_loss, dev_acc * 100))\n",
        "    print('dev Prec: {0} | dev Reca: {1}'.format(dev_prec, dev_reca))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cptMv4VZ95m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "REPORTS_DIR = f'{BASE_REPORTS_DIR}/abusive/'\n",
        "if not os.path.exists(REPORTS_DIR):\n",
        "    os.makedirs(REPORTS_DIR)\n",
        "dev_data  = data.TabularDataset(\n",
        "            path=\"drive/My Drive/Colab Notebooks/experiments/data/fastexttagger/abusivedev.tsv\",\n",
        "            format=\"tsv\", fields=datafields)\n",
        "dev_iterator = data.BucketIterator(dev_data,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True)\n",
        "\n",
        "for epoch in range(20):#\n",
        "    model.load_state_dict(torch.load(f'{OUTPUT_DIR}model_{epoch}.pt'))\n",
        "    dev_loss, dev_acc, dev_prec, dev_reca, tn, tp, fp, fn = evaluate(model, dev_iterator, criterion)\n",
        "    try:\n",
        "        pos_prec = tp / (tp +fp) \n",
        "        pos_reca = tp / (tp + fn)\n",
        "        neg_prec = tn / (tn + fn)\n",
        "        neg_reca = tn / (tn + fp)\n",
        "        pos_f1 = (2 * ((pos_prec * pos_reca) / (pos_prec + pos_reca)))\n",
        "        neg_f1 = (2 * ((neg_prec * neg_reca) / (neg_prec + neg_reca)))\n",
        "        macro_f1 = (pos_f1 + neg_f1) / 2\n",
        "    except ZeroDivisionError:\n",
        "        macro_f1 = 'zerodevision'\n",
        "\n",
        "\n",
        "    with open(f'{REPORTS_DIR}epoch{epoch}.txt', 'w') as f:\n",
        "        print('dev Loss: {0} | dev Acc: {1}'.format(dev_loss, dev_acc * 100), file=f)\n",
        "        print('dev Prec: {0} | dev Reca: {1}'.format(dev_prec, dev_reca), file=f)\n",
        "        print('tp: {0} | tn: {1} | fp: {2} | fn: {3}'.format(tp,tn,fp,fn), file=f)\n",
        "       print('Macro_F1: {0}'.format(macro_f1), file=f)\n",
        "   print('Epoch: {0}'.format(epoch))\n",
        "   print('dev Loss: {0} | dev Acc: {1}'.format(dev_loss, dev_acc * 100))\n",
        "   print('dev Prec: {0} | dev Reca: {1}'.format(dev_prec, dev_reca))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}