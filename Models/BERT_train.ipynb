{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1niK0BnEVALv",
        "colab_type": "text"
      },
      "source": [
        "Script that fine-tunes a BERT language model to evaluate created datasets\n",
        "\n",
        "based on https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S8rcX9x6sU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "!pip install transformers==2.1.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90mFLz7V6sMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import torch\n",
        "import pickle\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from tqdm import tqdm, trange\n",
        "import os\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
        "from transformers.optimization import AdamW, WarmupLinearSchedule\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import numpy as np\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ6RnnwH7HvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class BinaryClassificationProcessor(DataProcessor):\n",
        "    \"\"\"Processor for binary classification dataset.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[3]\n",
        "            label = line[1]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "def convert_example_to_feature(example_row):\n",
        "    # return example_row\n",
        "    example, label_map, max_seq_length, tokenizer, output_mode = example_row\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "    segment_ids = [0] * len(tokens)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    padding = [0] * (max_seq_length - len(input_ids))\n",
        "    input_ids += padding\n",
        "    input_mask += padding\n",
        "    segment_ids += padding\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    if output_mode == \"classification\":\n",
        "        label_id = label_map[example.label]\n",
        "    elif output_mode == \"regression\":\n",
        "        label_id = float(example.label)\n",
        "    else:\n",
        "        raise KeyError(output_mode)\n",
        "\n",
        "    return InputFeatures(input_ids=input_ids,\n",
        "                         input_mask=input_mask,\n",
        "                         segment_ids=segment_ids,\n",
        "                         label_id=label_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwuHoBlg7agU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger()\n",
        "csv.field_size_limit(2147483647)\n",
        "BERT_MODEL = 'bert-base-cased'\n",
        "TASK_NAME = 'test' #select task to evaluate\n",
        "BASE_PATH =\"drive/My Drive/Colab Notebooks/experiments\"\n",
        "OUTPUT_DIR = f'{BASE_PATH}/outputs/{TASK_NAME}/'\n",
        "DATA_DIR = f'{BASE_PATH}/data/BERT/new/abusive' #select data repository. Should contain train.tsv and test.tsv\n",
        "CACHE_DIR = 'cache/'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "MAX_SEQ_LENGTH = 128\n",
        "TRAIN_BATCH_SIZE = 24\n",
        "EVAL_BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 5\n",
        "RANDOM_SEED = 42\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "WARUMUP_STEPS = 100\n",
        "\n",
        "OUTPUT_MODE = 'classification'\n",
        "CONFIG_NAME = \"config.json\"\n",
        "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
        "\n",
        "output_mode = OUTPUT_MODE\n",
        "cache_dir = CACHE_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTe0AYxX7mYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor = BinaryClassificationProcessor()\n",
        "train_examples = processor.get_train_examples(DATA_DIR)\n",
        "train_examples_len = len(train_examples)\n",
        "label_list = processor.get_labels() # [0, 1] for binary classification\n",
        "num_labels = len(label_list)\n",
        "num_train_optimization_steps = int(\n",
        "    train_examples_len / TRAIN_BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS\n",
        "print('num_train_optimization_steps: {0}'.format(num_train_optimization_steps))\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "label_map = {label: i for i, label in enumerate(label_list)}\n",
        "train_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in train_examples]\n",
        "process_count = cpu_count() - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqMUOpT97sLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Preparing to convert {train_examples_len} examples..')\n",
        "print(f'Spawning {process_count} processes..')\n",
        "with Pool(process_count) as p:\n",
        "    train_features = list(tqdm(p.imap(convert_example_to_feature, train_examples_for_processing), total=train_examples_len))\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, cache_dir=CACHE_DIR, num_labels=num_labels)\n",
        "model.to(device)\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "                     \n",
        "optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                  lr = LEARNING_RATE,\n",
        "                  correct_bias=True)\n",
        "scheduler = WarmupLinearSchedule(optimizer,\n",
        "                                 warmup_steps= WARUMUP_STEPS,\n",
        "                                 t_total=num_train_optimization_steps)\n",
        "    \n",
        "    \n",
        "global_step = 0\n",
        "nb_tr_steps = 0\n",
        "tr_loss = 0\n",
        "print(\"***** Running training *****\")\n",
        "print(\"  Num examples = %d\", train_examples_len)\n",
        "print(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
        "print(\"  Num steps = %d\", num_train_optimization_steps)\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "if OUTPUT_MODE == \"classification\":\n",
        "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
        "elif OUTPUT_MODE == \"regression\":\n",
        "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lZe9FPO7snp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)\n",
        "model.train()\n",
        "epoch = 0\n",
        "if epoch != 0: #enables continue previous training run if interupted\n",
        "    continue_model = OUTPUT_DIR + 'epoch_{0}/pytorch_model.bin'.format(epoch)\n",
        "    model.load_state_dict(torch.load(continue_model))\n",
        "for x in trange(int(NUM_TRAIN_EPOCHS) - epoch, desc=\"Epoch\"):\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, label_ids = batch\n",
        "\n",
        "        logits = model(input_ids, segment_ids, input_mask, labels=None)[0]\n",
        "        if OUTPUT_MODE == \"classification\":\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
        "        elif OUTPUT_MODE == \"regression\":\n",
        "            loss_fct = MSELoss()\n",
        "            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "\n",
        "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
        "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
        "\n",
        "        loss.backward()\n",
        "        print(\"\\r%f\" % loss, end='')\n",
        "        \n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "    epoch += 1\n",
        "    OUTPUT_DIR_EPOCH = OUTPUT_DIR + 'epoch_{0}/'.format(epoch)\n",
        "    if os.path.exists(OUTPUT_DIR_EPOCH) and os.listdir(OUTPUT_DIR_EPOCH):\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(OUTPUT_DIR))\n",
        "    if not os.path.exists(OUTPUT_DIR_EPOCH):\n",
        "        os.makedirs(OUTPUT_DIR_EPOCH)\n",
        "    if not os.path.exists(OUTPUT_DIR + 'best/'):\n",
        "        os.makedirs(OUTPUT_DIR + 'best/')\n",
        "    output_model_file = os.path.join(OUTPUT_DIR_EPOCH, WEIGHTS_NAME)\n",
        "    output_config_file = os.path.join(OUTPUT_DIR_EPOCH, CONFIG_NAME)\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "    model_to_save.config.to_json_file(output_config_file)\n",
        "    tokenizer.save_vocabulary(OUTPUT_DIR_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}