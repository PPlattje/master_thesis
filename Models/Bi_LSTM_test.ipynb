{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bi-LSTM_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0qkK-K8Aknk",
        "colab_type": "text"
      },
      "source": [
        "Script testing BI-LSTM Models based on polarised or pre-trained embeddings\n",
        "\n",
        "Based on the following tutorials:\n",
        "\n",
        "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
        "\n",
        "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ob4cn8j2nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8k94YOoRqbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import random\n",
        "import time\n",
        "from torchtext import data\n",
        "from torchtext.vocab import Vectors\n",
        "import fasttext\n",
        "import os\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUMyhGg4RvYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_accuracy(rounded_preds, gold):\n",
        "    \"\"\"returns accuracy per batch.\"\"\"\n",
        "    #round to closest integer, cause binary classification\n",
        "    correct = 0\n",
        "    for x in range(len(rounded_preds)):\n",
        "        if rounded_preds[x] == gold[x]:\n",
        "            correct += 1\n",
        "        \n",
        "    acc = correct / len(rounded_preds)\n",
        "    return acc\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"count number of trainable parameters\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def progress_tracker(total, current, start_time):\n",
        "    per = (current / total) * 100\n",
        "    global milestone\n",
        "    cminutes, cseconds = epoch_time(start_time, time.time())\n",
        "    ctime = '{0}m {1}s'.format(cminutes,cseconds)\n",
        "    if per > 90:\n",
        "        if milestone != 90:\n",
        "            print('Progress training: 90% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 90\n",
        "    elif per > 80:\n",
        "        if milestone != 80:\n",
        "            print('Progress training: 80% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 80\n",
        "    elif per > 70:\n",
        "        if milestone != 70:\n",
        "            print('Progress training: 70% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 70\n",
        "    elif per > 60:\n",
        "        if milestone != 60:\n",
        "            print('Progress training: 60% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 60\n",
        "    elif per > 50:\n",
        "        if milestone != 50:\n",
        "            print('Progress training: 50% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 50\n",
        "    elif per > 40:\n",
        "        if milestone != 40:\n",
        "            print('Progress training: 40% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 40\n",
        "    elif per > 30:\n",
        "        if milestone != 30:\n",
        "            print('Progress training: 30% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 30\n",
        "    elif per > 20:\n",
        "        if milestone != 20:\n",
        "            print('Progress training: 20% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 20\n",
        "    elif per > 10:\n",
        "        if milestone != 10:\n",
        "            print('Progress training: 10% | current runtime: {0}'.format(ctime))\n",
        "            milestone = 10\n",
        "\n",
        "\n",
        "def calc_metrics(rounded_preds, y):\n",
        "    \"\"\"returns F1 score per batch.\"\"\"\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    tn = 0\n",
        "    for x in range(len(rounded_preds)):\n",
        "        if rounded_preds[x] == 1:\n",
        "            if y[x] == 1:\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            if y[x] == 1:\n",
        "                fn += 1\n",
        "            if y[x] == 0:\n",
        "                tn += 1\n",
        "    return tp, fp, fn, tn\n",
        "            \n",
        "    \n",
        "def evaluate(model, dev_iterator, criterion):\n",
        "    \"\"\"evaluate the model (no updateing model) \"\"\"\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    model.eval() #deactivate training (no updating model)\n",
        "    with torch.no_grad():\n",
        "        with open(f'{REPORTS_DIR}epoch{epoch}_preds.txt', 'w') as predfile:\n",
        "            ID = 0\n",
        "            predwriter = csv.writer(predfile, delimiter='\\t')\n",
        "            predwriter.writerow(['ID', 'prediction', 'gold standard'])\n",
        "            for batch in dev_iterator:\n",
        "                text, text_lengths = batch.text\n",
        "                predictions = model(text, text_lengths)\n",
        "                rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "                for x in range(len(batch)):\n",
        "                    predwriter.writerow([ID, rounded_preds, batch.label])\n",
        "                    ID +=1\n",
        "                loss = criterion(predictions, batch.label)\n",
        "                acc = calc_accuracy(rounded_preds, batch.label)\n",
        "                batch_tp, batch_fp, batch_fn, batch_tn = calc_metrics(rounded_preds, batch.label)\n",
        "                tn += batch_tn\n",
        "                tp += batch_tp\n",
        "                fp += batch_fp\n",
        "                fn += batch_fn\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc\n",
        "    if tp != 0:    \n",
        "        prec = tp / (tp + fp)\n",
        "        reca = tp / (tp +fn)\n",
        "    else:\n",
        "        prec = 'no TP (tp:{0},fp:{1},tn:{2},fn:{3})'.format(tp,fp,tn,fn)\n",
        "        reca = 'no TP'\n",
        "    return epoch_loss / len(dev_iterator), epoch_acc / len(dev_iterator), prec, reca, tn, tp, fp, fn\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "    \n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    \"\"\"BI-LSTM Neural Network Model\"\"\"\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx): \n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True,\n",
        "                           num_layers=n_layers, dropout=dropout)\n",
        "        self.out = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #packing ensured we do not process the the padded elements\n",
        "        # this makes it faster\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout       \n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))          \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "        return self.out(hidden.squeeze(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nq9nIObRz8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_PATH =\"drive/My Drive/Colab Notebooks/experiments\"\n",
        "TASK_NAME = '' #fill in name of task to evaluate\n",
        "OUTPUT_DIR = f'{BASE_PATH}/outputs/{TASK_NAME}/'\n",
        "BASE_REPORTS_DIR = f'{BASE_PATH}/reports/{TASK_NAME}_evaluation_report/test'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    raise ValueError(\"Output directory ({}) already does not exist.\".format(OUTPUT_DIR))\n",
        "if not os.path.exists(BASE_REPORTS_DIR):\n",
        "   os.makedirs(BASE_REPORTS_DIR)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "SEED = 111 \n",
        "torch.manual_seed(SEED)\n",
        "TEXT = data.Field(include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "datafields = [(\"text\", TEXT),\n",
        "             (\"label\", LABEL)]\n",
        "#select training data corresponding to test data, to build right vocabulary size             \n",
        "train_data  = data.TabularDataset( \n",
        "            path=\"drive/My Drive/Colab Notebooks/experiments/data/fastexttagger/hatetrain.tsv\",\n",
        "            format=\"tsv\", fields=datafields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-z2Hy7kBBeE",
        "colab_type": "text"
      },
      "source": [
        "Select pre-trained or polarised embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygik2CYsNQqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('loading vectors')\n",
        "#pre-trained embeddings:\n",
        "vec = Vectors(name='pretrained_embeddings.vec', cache='drive/My Drive/Colab Notebooks/embeddings')\n",
        "\n",
        "#polarised embeddings:\n",
        "#vec = Vectors(name='drive/My Drive/Colab Notebooks/embeddings/abusive_embeddings.vec', cache='/content/')\n",
        "\n",
        "print('building vocabulary')\n",
        "TEXT.build_vocab(train_data, unk_init = torch.Tensor.normal_,\n",
        "                 vectors = vec)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTqvc40dR4Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300 #should be same as embedding size\n",
        "HIDDEN_DIM = 300 \n",
        "OUTPUT_DIM = 1 \n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "BATCH_SIZE = 1\n",
        "print('creating model')\n",
        "model = Classifier(INPUT_DIM,EMBEDDING_DIM,HIDDEN_DIM,OUTPUT_DIM,N_LAYERS,DROPOUT,PAD_IDX)\n",
        "model.to(device)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "print('preparing model')\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = criterion.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww0EJV9BzT_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "REPORTS_DIR = f'{BASE_REPORTS_DIR}/test/'\n",
        "print(REPORTS_DIR)\n",
        "if not os.path.exists(REPORTS_DIR):\n",
        "    os.makedirs(REPORTS_DIR)\n",
        "test_data  = data.TabularDataset( #select test data\n",
        "            path=\"drive/My Drive/Colab Notebooks/experiments/data/fastexttagger/hatetest.tsv\",\n",
        "            format=\"tsv\", fields=datafields)\n",
        "test_iterator = data.BucketIterator(test_data,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    train=False,\n",
        "    sort=False,\n",
        "    shuffle=None,\n",
        "    )\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.load_state_dict(torch.load(f'{OUTPUT_DIR}model_{epoch}.pt'))\n",
        "    test_loss, test_acc, test_prec, test_reca, tn, tp, fp, fn = evaluate(model, test_iterator, criterion)\n",
        "    try:\n",
        "        pos_prec = tp / (tp +fp) \n",
        "        pos_reca = tp / (tp + fn)\n",
        "        neg_prec = tn / (tn + fn)\n",
        "        neg_reca = tn / (tn + fp)\n",
        "        pos_f1 = (2 * ((pos_prec * pos_reca) / (pos_prec + pos_reca)))\n",
        "        neg_f1 = (2 * ((neg_prec * neg_reca) / (neg_prec + neg_reca)))\n",
        "        macro_f1 = (pos_f1 + neg_f1) / 2\n",
        "    except ZeroDivisionError:\n",
        "        macro_f1 = 'zerodevision'\n",
        "\n",
        "\n",
        "    with open(f'{REPORTS_DIR}epoch{epoch}.txt', 'w') as f:\n",
        "        print('test Loss: {0} | test Acc: {1}'.format(test_loss, test_acc * 100), file=f)\n",
        "        print('test Prec: {0} | test Reca: {1}'.format(test_prec, test_reca), file=f)\n",
        "        print('tp: {0} | tn: {1} | fp: {2} | fn: {3}'.format(tp,tn,fp,fn), file=f)\n",
        "        print('Macro_F1: {0}'.format(macro_f1), file=f)\n",
        "    print('Epoch: {0}'.format(epoch))\n",
        "    print('test Loss: {0} | test Acc: {1}'.format(test_loss, test_acc * 100))\n",
        "    print('test Prec: {0} | test Reca: {1}'.format(test_prec, test_reca))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNf45A50cnO1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}